# RadOrderPad Test Documentation

This document provides an overview of the test suite for RadOrderPad, focusing on the validation and workflow tests.

## Table of Contents

1. [LLM Validation Tests](#llm-validation-tests)
2. [Comprehensive Workflow Tests](#comprehensive-workflow-tests)
3. [Clinical Workflow Simulation](#clinical-workflow-simulation)
4. [End-to-End Scenario Tests](#end-to-end-scenario-tests)
5. [Other Tests](#other-tests)
6. [Running Tests](#running-tests)
7. [Interpreting Test Results](#interpreting-test-results)

## LLM Validation Tests

### `tests/llm-validation-flow-test.js`

This test verifies the integration between multiple LLMs in the validation process:

- **Purpose**: Tests the complete validation flow using real API calls with all three LLMs working together
- **Flow**:
  1. Grok generates the initial dictation based on a prompt template
  2. Claude validates the dictation and provides feedback
  3. If validation fails or needs clarification, GPT generates a clarification
  4. Claude validates the clarification
- **Test Categories**:
  - Category A: Blatantly wrong cases (missing indication, contradictory info, mismatched study)
  - Category B: Cases that require one clarification (vague symptoms, missing duration, incomplete history)
  - Category C: Cases that are correct immediately (appropriate neurological case, appropriate MSK case, multiple studies)
- **Run Method**: `batch-files/run-llm-validation-tests.bat` (with options for fixed or random test cases)
- **Key Features**:
  - Tests the basic integration between the three LLMs
  - Verifies that each LLM can perform its specific role in the workflow
  - Includes a single round of clarification, not the full multi-attempt workflow

### `tests/prompt-templates.js`

- **Purpose**: Provides prompt templates for the LLM validation tests
- **Content**: Contains templates for different test scenarios (missing indication, contradictory info, etc.)
- **Usage**: Used by `llm-validation-flow-test.js` to generate test cases

## Comprehensive Workflow Tests

### `tests/e2e/run_comprehensive_tests.js`

This test verifies the complete end-to-end workflow with multiple validation attempts and override:

- **Purpose**: Tests the full clinical workflow from initial order to final submission
- **Flow**:
  1. Initial order entry and first validation attempt
  2. Second validation attempt
  3. Third validation attempt
  4. Physician override (only available after 3rd attempt)
  5. Admin processing (skipped in current implementation)
  6. Radiology verification (skipped in current implementation)
- **Test Cases**: Defined in `tests/e2e/comprehensive_workflow_test_cases.md`
- **Run Method**: `batch-files/run-comprehensive-workflow-tests.bat` (with optional test number parameter)
- **Key Features**:
  - Tests the complete multi-attempt validation workflow
  - Verifies that the override process works correctly
  - Checks that primary ICD-10 codes are correctly identified
  - Simulates different user roles (physician, admin, radiologist)

### `tests/e2e/comprehensive_workflow_test_cases.md`

- **Purpose**: Defines test cases for the comprehensive workflow tests
- **Content**: Contains 10 detailed test cases with expected outcomes
- **Usage**: Used by `run_comprehensive_tests.js` to run specific test scenarios

## Clinical Workflow Simulation

### `tests/clinical-workflow-simulation.js`

This test simulates a realistic clinical workflow with LLMs generating all content:

- **Purpose**: Simulates a realistic clinical environment with randomly generated cases
- **Flow**:
  1. Grok generates a random, brief dictation (as if from a busy clinical environment)
  2. Claude validates the dictation and provides feedback
  3. GPT generates a clarification response (as if from a busy physician)
  4. Claude validates the clarification
  5. If needed, GPT generates a second clarification
  6. Claude validates the second clarification
  7. If needed, GPT generates an override justification
  8. Claude processes the override
- **Run Method**: `batch-files/run-clinical-workflow-simulation.bat` (prompts for number of cases to run)
- **Key Features**:
  - Fully automated end-to-end testing with no hard-coded test cases
  - All content (dictations, clarifications, overrides) generated by LLMs
  - Simulates realistic clinical scenarios with time constraints
  - Tests the complete multi-attempt validation workflow with override
  - Provides detailed results and statistics on validation outcomes

## End-to-End Scenario Tests

### `tests/e2e/scenario-*.js` Files

These tests verify specific end-to-end scenarios:

- **`scenario-a-successful-validation.js`**: Tests successful validation on the first attempt
- **`scenario-b-validation-override.js`**: Tests the override process after failed validation
- **`scenario-c-admin-finalization.js`**: Tests the admin processing workflow
- **`scenario-d-radiology-workflow.js`**: Tests the radiologist workflow
- **`scenario-e-connection-request.js`**: Tests connection requests between organizations
- **`scenario-f-user-invite.js`**: Tests user invitation process
- **`scenario-g-file-upload.js`**: Tests file upload functionality
- **Run Method**: `batch-files/run-e2e-tests.bat`

## Other Tests

### `tests/billing-checkout.test.js`

- **Purpose**: Tests the billing and checkout functionality
- **Run Method**: `batch-files/run-billing-tests.bat`

### `tests/file-upload-test.js` and `tests/file-upload.test.js`

- **Purpose**: Tests file upload functionality
- **Run Method**: `batch-files/run-file-upload-tests.bat`

### `tests/stripe-webhooks.test.js`

- **Purpose**: Tests Stripe webhook integration
- **Run Method**: `batch-files/run-stripe-webhook-tests.bat`

## Running Tests

### LLM Validation Tests

```bash
# Run with fixed test cases
batch-files\run-llm-validation-tests.bat
# Select option 1 when prompted

# Run with random test cases
batch-files\run-llm-validation-tests.bat
# Select option 2 when prompted
```

### Comprehensive Workflow Tests

```bash
# Run all test cases
batch-files\run-comprehensive-workflow-tests.bat

# Run a specific test case (e.g., test case 1)
batch-files\run-comprehensive-workflow-tests.bat 1
```

### Clinical Workflow Simulation

```bash
# Run the clinical workflow simulation
batch-files\run-clinical-workflow-simulation.bat
# Enter the number of test cases to run when prompted
```

### End-to-End Tests

```bash
# Run all E2E tests
batch-files\run-e2e-tests.bat
```

### Other Tests

```bash
# Run billing tests
batch-files\run-billing-tests.bat

# Run file upload tests
batch-files\run-file-upload-tests.bat

# Run Stripe webhook tests
batch-files\run-stripe-webhook-tests.bat
```

## Interpreting Test Results

### LLM Validation Test Results

Test results are stored in the `test-results/llm-validation` directory:

- **Individual test case results**: Files like `vague_symptoms-result.json` contain detailed information about each test case, including:
  - The prompt used to generate the dictation
  - The dictation generated by Grok
  - The validation result from Claude
  - The clarification generated by GPT (if applicable)
  - The final validation result
  - Whether the test passed or failed

- **Summary results**: The `summary.json` file contains an overview of all test cases, including:
  - Total number of tests run
  - Number of tests passed and failed
  - Success rate
  - Breakdown by category (A, B, C)

A typical successful test case will show:
1. Grok generating a dictation based on the prompt
2. Claude validating the dictation with the expected status
3. If needed, GPT generating a clarification
4. Claude validating the clarification

### Comprehensive Workflow Test Results

Test results are stored in the `test-results` directory with filenames like `comprehensive-test-results-2025-04-16T17-59-14.013Z.json`:

- These files contain information about each test case, including:
  - Whether the test passed or failed
  - The error message if the test failed
  - The test case name

The comprehensive workflow tests verify:
1. The initial validation works correctly
2. Multiple validation attempts are handled properly
3. The override process works as expected
4. Primary ICD-10 codes are correctly identified

### Clinical Workflow Simulation Results

Test results are stored in the `test-results/clinical-workflow` directory:

- **Individual case results**: Files like `case-1-result.json` contain detailed information about each test case, including:
  - The dictation generated by Grok
  - The validation result from Claude
  - The clarification generated by GPT
  - The clarification validation result
  - The second clarification (if needed)
  - The override justification (if needed)
  - The final status of the case

- **Summary results**: The `summary.json` file contains an overview of all cases, including:
  - Total number of cases run
  - Breakdown by final status (validated_first_attempt, validated_after_clarification, etc.)
  - Total duration

This test provides the most realistic simulation of the clinical workflow, as it uses LLMs to generate all content rather than using hard-coded test cases.

### Common Test Failures

1. **Validation Status Mismatch**: The most common failure is a mismatch between the expected validation status and the actual status. This can happen when:
   - The test case expects "needs_clarification" but the system returns "inappropriate"
   - The test case expects "validation_failed" but the system returns "needs_clarification"

2. **Missing Primary ICD-10 Code**: This failure occurs when the system doesn't identify a primary ICD-10 code in the validation result.

3. **API Key Issues**: If the API keys for the LLMs are not properly configured, the tests will fail with errors about missing API keys.

## Test Environment Setup

Before running tests, ensure:

1. The database is properly set up and populated
2. API keys for all LLMs are configured in the `.env` file:
   ```
   ANTHROPIC_API_KEY=your_anthropic_api_key
   GROK_API_KEY=your_grok_api_key
   OPENAI_API_KEY=your_openai_api_key
   ```
3. The local server is running (if testing against a local server)

## Troubleshooting

- **Missing API Keys**: If you see errors about missing API keys, check your `.env` file
- **Database Connection Issues**: Verify database connection parameters in `test-config.js`
- **Test Failures**: Check the detailed test results in the `test-results` directory
- **Validation Status Mismatch**: This may indicate a discrepancy between the test case definition and the actual expected behavior. Review the test case and consider updating it if the system's response is actually correct.